{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1edda9-ef2f-416f-97ed-c8c5bedccaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/hal9000/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from tqdm import tqdm \n",
    "from collections import Counter,OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import string  \n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def flatten_comprehension(matrix):\n",
    "    #https://realpython.com/python-flatten-list/\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "# Load data\n",
    "behaviors = pd.read_csv('./small_training_data/behaviors.tsv', delimiter='\\t', header=None)\n",
    "news = pd.read_csv('./small_training_data/news.tsv', delimiter='\\t', header=None)\n",
    "\n",
    "# Naming columns\n",
    "behaviors.columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "news.columns = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "\n",
    "\n",
    "# Extracting clicked news from behaviors, this is a column of lists of the clicked news (tagget with 1) for each impression\n",
    "behaviors['clicked_news'] = behaviors['impressions'].apply(lambda x: [imp.split('-')[0] for imp in x.split() if imp.split('-')[1] == '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf495416-6fd4-489d-8823-35e01fdda570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|####################################################################################################################| 50000/50000 [03:52<00:00, 215.03it/s]\n"
     ]
    }
   ],
   "source": [
    "#define unique users and their read articles\n",
    "unique_users_series = behaviors[\"user_id\"].drop_duplicates()\n",
    "unique_users_dict = {}  #user_id coupled to news_articles\n",
    "\n",
    "#aggregate all read articles and couple it to the user in the unique_users_frame\n",
    "i = 0\n",
    "for user_id in tqdm(unique_users_series,ascii=True):\n",
    "    user_specific_behavior = behaviors[behaviors[\"user_id\"] == user_id]\n",
    "    try:\n",
    "        history_list = flatten_comprehension([elem.split(\" \") for elem in user_specific_behavior[\"history\"]])\n",
    "    except:\n",
    "        pass \n",
    "    clicked_list = flatten_comprehension([elem for elem in user_specific_behavior[\"clicked_news\"]])\n",
    "    all_read_articles = history_list + clicked_list\n",
    "    unique_users_dict[user_id] = all_read_articles\n",
    "\n",
    "\n",
    "with open('unique_users.pkl', 'wb') as file: #serialize so we don't have to do this inefficient code again\n",
    "    pickle.dump(unique_users_dict,file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ca38519-5340-4ed7-b86a-2b970a48f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the embedding. (what words we look for)\n",
    "\n",
    "def remove_punctuation(sentence:str) -> str:\n",
    "    sentence = sentence.replace(\"'\",\"\")\n",
    "    for elem in string.punctuation:\n",
    "        sentence = sentence.replace(elem,\" \")\n",
    "    return sentence\n",
    "\n",
    "def remove_stopwords(sentence:str)-> str:\n",
    "    stop_words = list(set(stopwords.words(\"english\")))\n",
    "    sentence= [ elem for elem in sentence.split(\" \") if elem not in stop_words]\n",
    "        \n",
    "    return \" \".join(sentence)\n",
    "\n",
    "def preprocess(sentence:str) -> Counter:\n",
    "    counter = Counter(remove_punctuation(remove_stopwords(sentence.lower())).split(\" \"))\n",
    "    counter[''] =0 \n",
    "    return counter\n",
    "\n",
    "\n",
    "# total_title = \"\"\n",
    "# for title in tqdm(news[\"title\"],ascii=True):\n",
    "#     total_title += \" \"+title \n",
    "# total_BOW = preprocess(total_title)\n",
    "\n",
    "\n",
    "# sorted_BOW = sorted(OrderedDict(total_BOW).items(),key=lambda x: x[1],reverse=True)\n",
    "# print(sorted_BOW)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5dfde93-a976-47ea-85ba-9757a163d108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|####################################################################################################################| 50000/50000 [01:28<00:00, 567.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding_words = [elem[0] for elem in sorted_BOW[:1000]]\n",
    "\n",
    "with open('unique_users.pkl', 'rb') as file:\n",
    "    users = pickle.load(file)\n",
    "\n",
    "user_BOW = {} #{\"user_id\":embedding,etc.}\n",
    "\n",
    "#compute user embeddings\n",
    "for user in tqdm(users,ascii=True):\n",
    "    news_ids = users[user]    \n",
    "    counter =preprocess(news[\"title\"][news[\"news_id\"].isin(news_ids)].map(str.lower).apply(lambda x: x +\" \").agg(\"sum\"))\n",
    "    \n",
    "    user_BOW[user] = counter\n",
    "    \n",
    "with open('userBOW.pkl', 'wb') as file: #serialize so we don't have to do this inefficient code again\n",
    "    pickle.dump(user_BOW,file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcfc4d58-8abc-4e55-990b-074644e84001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                 | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'former': 2, 'dead': 2, 'flight': 2, 'michigan': 2, 'state': 2, 'joe': 1, 'biden': 1, 'reportedly': 1, 'denied': 1, 'communion': 1, 'south': 1, 'carolina': 1, 'church': 1, 'stance': 1, 'abortion': 1, 'wedding': 1, 'photo': 1, 'canine': 1, 'best': 1, 'man': 1, 'captures': 1, 'deep': 1, 'dogs': 1, 'love': 1, 'truly': 1, 'wheel': 1, 'fortune': 1, 'guest': 1, 'delivers': 1, 'hilarious': 1, 'rails': 1, 'introduction': 1, 'three': 1, 'takeaways': 1, 'yankees': 1, 'alcs': 1, 'game': 1, '5': 1, 'victory': 1, 'astros': 1, 'us': 1, 'senator': 1, 'kay': 1, 'hagan': 1, '66': 1, 'robert': 1, 'evans': 1, 'chinatown': 1, 'producer': 1, 'paramount': 1, 'chief': 1, 'dies': 1, '89': 1, 'rosie': 1, 'odonnell': 1, 'barbara': 1, 'walters': 1, 'up': 1, 'speaking': 1, 'people': 1, 'right': 1, 'four': 1, 'attendants': 1, 'arrested': 1, 'miamis': 1, 'airport': 1, 'bringing': 1, 'thousands': 1, 'cash': 1, 'police': 1, 'say': 1, 'sends': 1, 'breakup': 1, 'tweet': 1, 'notre': 1, 'dame': 1, 'series': 1, 'goes': 1, 'hold': 1, 'happens': 1, 'oxygen': 1, 'mask': 1, 'inflate': 1, 'charles': 1, 'rogers': 1, 'football': 1, 'detroit': 1, 'lions': 1, 'star': 1, '38': 1, 'george': 1, 'kent': 1, 'top': 1, 'department': 1, 'ukraine': 1, 'expert': 1, 'helps': 1, 'democrats': 1, 'debunk': 1, 'gop': 1, 'theories': 1, '': 0})\n",
      "Counter()\n",
      "106\n",
      "    user1   user2  similarity\n",
      "0  U13740  U13740         1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('userBOW.pkl', 'rb') as file:\n",
    "    user_BOW = pickle.load(file)\n",
    "\n",
    "df = pd.DataFrame({\"user1\":[],\"user2\":[],\"similarity\":[]})\n",
    "\n",
    "for user1 in tqdm(user_BOW,ascii=True):\n",
    "    for user2 in user_BOW:\n",
    "        print((user_BOW[user1] & user_BOW[user2]).total())\n",
    "        similarity = (user_BOW[user1] - user_BOW[user2]).total()        \n",
    "        df_row = pd.DataFrame({\"user1\":[user1],\"user2\":[user2],\"similarity\":[1]})\n",
    "        print(pd.concat([df, df_row], join=\"inner\")) \n",
    "\n",
    "        break\n",
    "    break \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726bb80-7a0f-4c5a-9f3d-9a28bf526153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE \n",
    "\n",
    "instead of cosine similarity we use Jaccard similarity J (https://stats.stackexchange.com/questions/289400/quantify-the-similarity-of-bags-of-words)\n",
    "because the BOW embeddings are really sparse except if you use to common words ( \"a\", \"is\", \"of\",etc) \n",
    "This would be counter intuitive to use. \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
