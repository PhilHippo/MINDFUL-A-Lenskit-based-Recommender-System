{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc1edda9-ef2f-416f-97ed-c8c5bedccaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/hal9000/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'has', 'being', 'him', 'with', 'been', \"you'd\", 'myself', 'couldn', 'does', 'are', 'against', 'how', 'theirs', 'do', 'both', 'under', \"won't\", 'until', 'mustn', 'each', 'ain', 'yours', \"wasn't\", 'haven', 'or', \"shouldn't\", 'will', 'during', 'between', 'whom', 'where', 'below', 'ours', 'for', 'over', 'its', 'very', 'yourselves', 'should', 'having', 'at', 'not', 'out', 'here', \"mightn't\", 'ourselves', 'such', 'few', \"aren't\", \"hadn't\", 'doing', 'their', 'i', 'some', 'shan', 'as', 'the', 'she', \"shan't\", 'did', 'while', 'which', 'didn', \"didn't\", 'than', 'hadn', 's', 'of', \"should've\", 'doesn', 'only', \"it's\", \"hasn't\", 'you', 'yourself', 'any', 'off', 'when', 'am', 'but', 'don', 'be', \"you've\", 'is', 'itself', 'most', 'herself', 'into', 'weren', 'so', 'was', \"couldn't\", \"you'll\", 'to', 'down', 'just', 'll', 'in', 't', 'can', 'because', 'by', 'more', 'they', 'own', 'he', 'up', 'after', 'her', 'himself', 'all', 'hasn', 'wouldn', 'my', \"that'll\", 'further', 'those', 're', 'on', \"isn't\", 'y', 'o', 'what', 'mightn', 'needn', 'shouldn', \"wouldn't\", \"doesn't\", 'this', 'had', 'aren', 'nor', 'isn', 've', \"don't\", \"haven't\", \"mustn't\", 'that', 'd', 'his', 'wasn', \"you're\", 'it', 'there', 'won', 'who', 'now', 'other', 'about', 'then', 'm', 'themselves', 'these', 'from', 'too', 'above', 'no', 'were', 'ma', \"weren't\", 'them', \"she's\", 'a', \"needn't\", 'if', 'hers', 'an', 'we', 'our', 'have', 'same', 'again', 'your', 'once', 'through', 'why', 'me', 'before', 'and'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from tqdm import tqdm \n",
    "from collections import Counter,OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import string  \n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def flatten_comprehension(matrix):\n",
    "    #https://realpython.com/python-flatten-list/\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "# Load data\n",
    "behaviors = pd.read_csv('./small_training_data/behaviors.tsv', delimiter='\\t', header=None)\n",
    "news = pd.read_csv('./small_training_data/news.tsv', delimiter='\\t', header=None)\n",
    "\n",
    "# Naming columns\n",
    "behaviors.columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "news.columns = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "\n",
    "\n",
    "# Extracting clicked news from behaviors, this is a column of lists of the clicked news (tagget with 1) for each impression\n",
    "behaviors['clicked_news'] = behaviors['impressions'].apply(lambda x: [imp.split('-')[0] for imp in x.split() if imp.split('-')[1] == '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf495416-6fd4-489d-8823-35e01fdda570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################################################################| 50000/50000 [04:11<00:00, 198.97it/s]\n"
     ]
    }
   ],
   "source": [
    "#define unique users and their read articles\n",
    "unique_users_series = behaviors[\"user_id\"].drop_duplicates()\n",
    "unique_users_dict = {}  #user_id coupled to news_articles\n",
    "\n",
    "#aggregate all read articles and couple it to the user in the unique_users_frame\n",
    "i = 0\n",
    "for user_id in tqdm(unique_users_series,ascii=True):\n",
    "    user_specific_behavior = behaviors[behaviors[\"user_id\"] == user_id]\n",
    "    try:\n",
    "        history_list = flatten_comprehension([elem.split(\" \") for elem in user_specific_behavior[\"history\"]])\n",
    "    except:\n",
    "        pass \n",
    "    clicked_list = flatten_comprehension([elem for elem in user_specific_behavior[\"clicked_news\"]])\n",
    "    all_read_articles = history_list + clicked_list\n",
    "    unique_users_dict[user_id] = all_read_articles\n",
    "\n",
    "\n",
    "with open('unique_users.pkl', 'wb') as file: #serialize so we don't have to do this inefficient code again\n",
    "    pickle.dump(unique_users_dict,file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca38519-5340-4ed7-b86a-2b970a48f319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|################################################################################################################| 51282/51282 [00:00<00:00, 2116133.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#define the embedding. (what words we look for)\n",
    "\n",
    "def remove_punctuation(sentence:str) -> str:\n",
    "    sentence = sentence.replace(\"'\",\"\")\n",
    "    for elem in string.punctuation:\n",
    "        sentence = sentence.replace(elem,\" \")\n",
    "    return sentence\n",
    "\n",
    "def remove_stopwords(sentence:str)-> str:\n",
    "    stop_words = list(set(stopwords.words(\"english\")))\n",
    "    sentence= [ elem for elem in sentence.split(\" \") if elem not in stop_words]\n",
    "        \n",
    "    return \" \".join(sentence)\n",
    "\n",
    "def preprocess(sentence:str) -> Counter:\n",
    "    counter = Counter(remove_punctuation(remove_stopwords(sentence.lower())).split(\" \"))\n",
    "    counter[''] =0 \n",
    "    return counter\n",
    "\n",
    "\n",
    "total_title = \"\"\n",
    "for title in tqdm(news[\"title\"],ascii=True):\n",
    "    total_title += \" \"+title \n",
    "total_BOW = preprocess(total_title)\n",
    "\n",
    "\n",
    "sorted_BOW = sorted(OrderedDict(total_BOW).items(),key=lambda x: x[1],reverse=True)\n",
    "print(sorted_BOW)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfde93-a976-47ea-85ba-9757a163d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_words = [elem[0] for elem in sorted_BOW[:500]]\n",
    "\n",
    "with open('unique_users.pkl', 'rb') as file:\n",
    "    users = pickle.load(file)\n",
    "\n",
    "user_BOW = {} #{\"user_id\":embedding,etc.}\n",
    "\n",
    "#compute user embeddings\n",
    "for user in tqdm(users,ascii=True):\n",
    "    news_ids = users[user]     \n",
    "    user_BOW[user] = Counter(news[\"title\"][news[\"news_id\"].isin(news_ids)].map(str.lower).apply(lambda x: x +\" \").agg(\"sum\").split(\" \"))\n",
    "    \n",
    "with open('userBOW.pkl', 'wb') as file: #serialize so we don't have to do this inefficient code again\n",
    "    pickle.dump(user_BOW,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc4d58-8abc-4e55-990b-074644e84001",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('userBOW.pkl', 'rb') as file:\n",
    "    user_BOW = pickle.load(file)\n",
    "\n",
    "ordered_userBOW = OrderedDict(user_BOW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726bb80-7a0f-4c5a-9f3d-9a28bf526153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE \n",
    "\n",
    "instead of cosine similarity we use Jaccard similarity J (https://stats.stackexchange.com/questions/289400/quantify-the-similarity-of-bags-of-words)\n",
    "because the BOW embeddings are really sparse except if you use to common words ( \"a\", \"is\", \"of\",etc) \n",
    "This would be counter intuitive to use. \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
