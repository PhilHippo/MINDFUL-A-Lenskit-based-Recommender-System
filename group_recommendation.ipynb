{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cc1edda9-ef2f-416f-97ed-c8c5bedccaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from tqdm import tqdm \n",
    "from collections import Counter,OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def flatten_comprehension(matrix):\n",
    "    #https://realpython.com/python-flatten-list/\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "# Load data\n",
    "behaviors = pd.read_csv('./small_training_data/behaviors.tsv', delimiter='\\t', header=None)\n",
    "news = pd.read_csv('./small_training_data/news.tsv', delimiter='\\t', header=None)\n",
    "\n",
    "# Naming columns\n",
    "behaviors.columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "news.columns = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "\n",
    "\n",
    "# Extracting clicked news from behaviors, this is a column of lists of the clicked news (tagget with 1) for each impression\n",
    "behaviors['clicked_news'] = behaviors['impressions'].apply(lambda x: [imp.split('-')[0] for imp in x.split() if imp.split('-')[1] == '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf495416-6fd4-489d-8823-35e01fdda570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#################################################| 50000/50000 [04:24<00:00, 189.29it/s]\n"
     ]
    }
   ],
   "source": [
    "#define unique users and their read articles\n",
    "unique_users_series = behaviors[\"user_id\"].drop_duplicates()\n",
    "unique_users_dict = {}  #user_id coupled to news_articles\n",
    "\n",
    "#aggregate all read articles and couple it to the user in the unique_users_frame\n",
    "i = 0\n",
    "for user_id in tqdm(unique_users_series,ascii=True):\n",
    "    user_specific_behavior = behaviors[behaviors[\"user_id\"] == user_id]\n",
    "    try:\n",
    "        history_list = flatten_comprehension([elem.split(\" \") for elem in user_specific_behavior[\"history\"]])\n",
    "    except:\n",
    "        pass \n",
    "    clicked_list = flatten_comprehension([elem for elem in user_specific_behavior[\"clicked_news\"]])\n",
    "    all_read_articles = history_list + clicked_list\n",
    "    unique_users_dict[user_id] = all_read_articles\n",
    "\n",
    "\n",
    "with open('unique_users.pkl', 'wb') as file: #serialize so we don't have to do this inefficient code again\n",
    "    pickle.dump(unique_users_dict,file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ca38519-5340-4ed7-b86a-2b970a48f319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|################################################################################################################################################| 51282/51282 [00:11<00:00, 4294.03it/s]\n"
     ]
    }
   ],
   "source": [
    "#define the embedding. (what words we look for)\n",
    "\n",
    "total_title = \"\"\n",
    "for title in tqdm(news[\"title\"],ascii=True):\n",
    "    total_title += \" \"+title \n",
    "\n",
    "total_BOW = Counter(total_title.lower().split(\" \"))\n",
    "sorted_BOW = sorted(OrderedDict(total_BOW).items(),key=lambda x: x[1],reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5dfde93-a976-47ea-85ba-9757a163d108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#################################################################################################################################################| 50000/50000 [01:09<00:00, 723.13it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_words = [elem[0] for elem in sorted_BOW[30:500]]\n",
    "\n",
    "with open('unique_users.pkl', 'rb') as file:\n",
    "    users = pickle.load(file)\n",
    "\n",
    "user_BOW = {} #{\"user_id\":embedding,etc.}\n",
    "\n",
    "#compute user embeddings\n",
    "for user in tqdm(users,ascii=True):\n",
    "    news_ids = users[user]     \n",
    "    user_BOW[user] = Counter(news[\"title\"][news[\"news_id\"].isin(news_ids)].map(str.lower).apply(lambda x: x +\" \").agg(\"sum\"))\n",
    "    print(user_BOW)\n",
    "    break\n",
    "\n",
    "\n",
    "# with open('userBOW.pkl', 'wb') as file: #serialize so we don't have to do this inefficient code again\n",
    "#     pickle.dump(user_BOW,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bcfc4d58-8abc-4e55-990b-074644e84001",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('userBOW.pkl', 'rb') as file:\n",
    "    user_BOW = pickle.load(file)\n",
    "\n",
    "ordered_userBOW = OrderedDict(user_BOW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726bb80-7a0f-4c5a-9f3d-9a28bf526153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE \n",
    "\n",
    "instead of cosine similarity we use Jaccard similarity J (https://stats.stackexchange.com/questions/289400/quantify-the-similarity-of-bags-of-words)\n",
    "because the BOW embeddings are really sparse except if you use to common words ( \"a\", \"is\", \"of\",etc) \n",
    "This would be counter intuitive to use. \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
