{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d800cf5",
   "metadata": {
    "id": "5d800cf5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lenskit.algorithms.user_knn import UserUser\n",
    "from lenskit.batch import predict\n",
    "from lenskit import batch, topn, util\n",
    "from lenskit import crossfold as xf\n",
    "from lenskit.algorithms import Recommender, Predictor\n",
    "from lenskit.algorithms.item_knn import ItemItem\n",
    "from lenskit.algorithms.basic import Bias\n",
    "from lenskit.metrics.predict import rmse\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d712341",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d712341",
    "outputId": "2a0470d8-5030-4945-e64e-1bf7b65ab567",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of news items: 48616\n",
      "Number of unique clicked news: 7307\n",
      "Number of unique users: 49445\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "behaviors = pd.read_csv('./small_training_data/behaviors.tsv', delimiter='\\t', header=None)\n",
    "news = pd.read_csv('./small_training_data/news.tsv', delimiter='\\t', header=None)\n",
    "\n",
    "# Naming columns\n",
    "behaviors.columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "news.columns = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "\n",
    "# Remove NaN values in the 'abstract' column\n",
    "news = news.dropna(subset=['abstract'])\n",
    "\n",
    "# Extracting clicked news from behaviors, this is a column of lists of the clicked news (tagget with 1) for each impression\n",
    "behaviors['clicked_news'] = behaviors['impressions'].apply(lambda x: [imp.split('-')[0] for imp in x.split() if imp.split('-')[1] == '1'])\n",
    "\n",
    "# Flattening the clicked news and associating with user_id, that means we divide the lists into one row for each clicked news\n",
    "clicked_news = behaviors.explode('clicked_news')[['user_id', 'clicked_news']].dropna()\n",
    "\n",
    "# Remove clicked news that were removed from the news DataFrame\n",
    "valid_news_ids = set(news['news_id'])\n",
    "# Remove clicked news that were removed from the news DataFrame\n",
    "valid_news_ids = set(news['news_id'])\n",
    "clicked_news = clicked_news[clicked_news['clicked_news'].isin(valid_news_ids)].copy()\n",
    "\n",
    "# Encoding user_id and news_id as categorical variables for memory and computation efficiency\n",
    "clicked_news['user_id'] = clicked_news['user_id'].astype(\"category\")\n",
    "clicked_news['clicked_news'] = clicked_news['clicked_news'].astype(\"category\")\n",
    "\n",
    "print(f\"Total number of news items: {news.shape[0]}\") #48616 unique news\n",
    "print(f\"Number of unique clicked news: {clicked_news['clicked_news'].nunique()}\") #7307 unique news have been clicked\n",
    "print(f\"Number of unique users: {clicked_news['user_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe63a10",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'news_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'news_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create categorical types without encoding them yet\u001b[39;00m\n\u001b[0;32m      6\u001b[0m clicked_news[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_cat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clicked_news[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m clicked_news[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews_id_cat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mclicked_news\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnews_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Creating mappings from original IDs to encoded IDs\u001b[39;00m\n\u001b[0;32m     10\u001b[0m id_to_user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(clicked_news[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_cat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcategories))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'news_id'"
     ]
    }
   ],
   "source": [
    "# Ensure 'user_id' and 'clicked_news' are strings\n",
    "clicked_news['user_id'] = clicked_news['user_id'].astype(str)\n",
    "clicked_news['clicked_news'] = clicked_news['clicked_news'].astype(str)\n",
    "\n",
    "# Create categorical types without encoding them yet\n",
    "clicked_news['user_id_cat'] = clicked_news['user_id'].astype(\"category\")\n",
    "clicked_news['news_id_cat'] = clicked_news['news_id'].astype(\"category\")\n",
    "\n",
    "# Creating mappings from original IDs to encoded IDs\n",
    "id_to_user = dict(enumerate(clicked_news['user_id_cat'].cat.categories))\n",
    "id_to_news = dict(enumerate(clicked_news['news_id_cat'].cat.categories))\n",
    "\n",
    "# Convert categories to codes (integer encoding)\n",
    "clicked_news['user_id'] = clicked_news['user_id_cat'].cat.codes\n",
    "clicked_news['news_id'] = clicked_news['news_id_cat'].cat.codes\n",
    "\n",
    "# Drop the additional categorical columns if they are not needed\n",
    "clicked_news = clicked_news.drop(columns=['user_id_cat', 'news_id_cat'])\n",
    "\n",
    "# Creating reverse mappings from original IDs to encoded IDs\n",
    "user_to_id = {v: k for k, v in id_to_user.items()}\n",
    "news_to_id = {v: k for k, v in id_to_news.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a2188",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f3a2188",
    "outputId": "1ad98875-9d95-4740-f3f7-ca979e305469"
   },
   "outputs": [],
   "source": [
    "# Create a sparse user-item interaction matrix\n",
    "interaction_matrix = coo_matrix((np.ones(clicked_news.shape[0]),\n",
    "                                 (clicked_news['user_id'], clicked_news['news_id'])))\n",
    "\n",
    "print(f\"users: {interaction_matrix.shape[0]} \\nitems: {interaction_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-GmRETVNBh-g",
   "metadata": {
    "id": "-GmRETVNBh-g"
   },
   "outputs": [],
   "source": [
    "interaction_matrix_csr = interaction_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bRWPvWPvDPjZ",
   "metadata": {
    "id": "bRWPvWPvDPjZ"
   },
   "outputs": [],
   "source": [
    "clicked_news_lenskit = clicked_news.rename(columns={'user_id': 'user', 'news_id': 'item'})\n",
    "\n",
    "clicked_news_lenskit['user'] = clicked_news_lenskit['user'].astype(int)\n",
    "clicked_news_lenskit['item'] = clicked_news_lenskit['item'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9492b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_news_lenskit['rating'] = np.ones(len(clicked_news_lenskit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clicked_news_lenskit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F75HqpP2CMyA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "F75HqpP2CMyA",
    "outputId": "49249d73-4c9e-45a1-e8f7-9df408d18966",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train User-User Collaborative Filtering Model\n",
    "user_user = UserUser(5, min_nbrs= 1)  # 15 neighbors, minimum 3 neighbors for prediction\n",
    "user_user.fit(clicked_news_lenskit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d89bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'user_id' and count the number of clicks per user\n",
    "user_click_counts = clicked_news.groupby('user_id').size().reset_index(name='num_clicks')\n",
    "\n",
    "# Sort users by the number of clicks in descending order and get the top 10\n",
    "top_users = user_click_counts.sort_values(by='num_clicks', ascending=False).head(10)\n",
    "\n",
    "# Map internal user IDs to real user IDs\n",
    "top_users['real_user_id'] = top_users['user_id'].map(id_to_user)\n",
    "\n",
    "# Display the top 10 users with real user IDs\n",
    "print(top_users[['real_user_id', 'num_clicks']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa749a2",
   "metadata": {},
   "source": [
    "Item_profiles matrix is in the Compressed Sparse Row (CSR) format, which is a memory-efficient way to store large sparse matrices. In each tuple (i,j): i corresponds to a news item and j corresponds to a specific word in the vocabulary from vectorizer.\n",
    "\n",
    "The TF-IDF value represents the importance of term j in the abstract of news item i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # or another number that suits your data\n",
    "\n",
    "# Fit and transform the abstracts to create item profiles\n",
    "item_profiles = vectorizer.fit_transform(news['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97659924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 517)\t1.0\n",
      "  (1, 368)\t1.0\n",
      "  (2, 585)\t0.5096714647312968\n",
      "  (2, 929)\t0.4937392329610019\n",
      "  (2, 957)\t0.5212708291507341\n",
      "  (2, 512)\t0.47406042921788183\n",
      "  (3, 596)\t0.36665808943657396\n",
      "  (3, 309)\t0.4406935092526457\n",
      "  (3, 401)\t0.33416629676258836\n",
      "  (3, 261)\t0.3629959351817992\n",
      "  (3, 975)\t0.41711744807412415\n",
      "  (3, 593)\t0.4080566359127731\n",
      "  (3, 510)\t0.2956699678737417\n",
      "  (4, 70)\t0.5522490166848188\n",
      "  (4, 40)\t0.4038523720269147\n",
      "  (4, 688)\t0.5213453133241538\n",
      "  (4, 368)\t0.5100228911076689\n"
     ]
    }
   ],
   "source": [
    "print(item_profiles[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0d72be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "#let's check what is the word mapped to id\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "print(terms[368])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "366b87b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract for news item N10051:\n",
      "\n",
      "From 2023 on, Volkswagen intends to produce 1.4 million of similar electric drive units annually.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and print the abstract of item 0\n",
    "original_news_id = id_to_news[1]\n",
    "\n",
    "abstract = news[news['news_id'] == original_news_id]['abstract'].values[0]\n",
    "print(f\"Abstract for news item {original_news_id}:\\n\\n{abstract}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d916a",
   "metadata": {},
   "source": [
    " # the above trained item profiles don't really match with the respective abstracts!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "_lSan5o_C4LH",
   "metadata": {
    "id": "_lSan5o_C4LH"
   },
   "outputs": [],
   "source": [
    "def get_clicked_news(real_user_id, clicked_news = clicked_news):\n",
    "    \"\"\"\n",
    "    Retrieve the news item indexes clicked by a specific user.\n",
    "    \n",
    "    Parameters:\n",
    "    - real_user_id (str): The original user ID from the dataset.\n",
    "    - clicked_news (DataFrame): The DataFrame containing user interactions with news items.\n",
    "                                 Expected columns are ['user_id', 'news_id'].\n",
    "    \n",
    "    Returns:\n",
    "    - list of str: A list containing the IDs of news items clicked by the specified user.\n",
    "    \n",
    "    Example:\n",
    "    >>> get_clicked_news('U53220', clicked_news)\n",
    "    [6583, 4676, 6715, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert real user ID to internal user ID\n",
    "    user_id = user_to_id[real_user_id]\n",
    "    \n",
    "    # Retrieve and return the clicked news IDs for the specified user\n",
    "    return clicked_news[clicked_news['user_id'] == user_id]['news_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7bf41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_partial_similarity(clicked_item_profiles, all_item_profiles):\n",
    "    # Compute the similarity between clicked items and all items\n",
    "    similarity_matrix = cosine_similarity(clicked_item_profiles, all_item_profiles)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e93d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_similar(news_id, similarity_matrix, n=10):\n",
    "    # Get the top-N similar items for a specific item\n",
    "    similar_items = np.argsort(similarity_matrix[news_id, :])[-n-1:-1][::-1]\n",
    "    return similar_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df97a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_user_user (user_id, user_user = user_user, clicked_news = clicked_news_lenskit, n=10):\n",
    "    \"\"\"\n",
    "    Recommend top-N items for a user using the UserUser collaborative filtering model.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_user: Trained UserUser collaborative filtering model.\n",
    "    - user_id: The internal ID of the user for whom to generate recommendations.\n",
    "    - all_item_ids: List of all possible item IDs to consider for recommendation.\n",
    "    - n: Number of recommendations to generate.\n",
    "    \n",
    "    Returns:\n",
    "    A DataFrame containing the top-N recommended items and their predicted ratings.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_item_ids = clicked_news['item'].unique()\n",
    "    \n",
    "    user_item_df = pd.DataFrame({\n",
    "    'user': [user_id] * len(all_item_ids),\n",
    "    'item': all_item_ids\n",
    "    })\n",
    "    \n",
    "    # Predict ratings for all user-item pairs\n",
    "    all_predictions = predict(user_user, user_item_df)\n",
    "    \n",
    "    # Select top-N items\n",
    "    top_items = all_predictions.nlargest(1000, 'prediction')\n",
    "    \n",
    "    return top_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7443192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(real_user_id, clicked_news = clicked_news, item_profiles = item_profiles, user_to_id = user_to_id, id_to_news = id_to_news, user_user = user_user, n=10, w_content=0.5, w_user_user=0.5):\n",
    "    \"\"\"\n",
    "    Recommend items for a user based on a hybrid approach combining collaborative filtering and content-based recommendations.\n",
    "    \n",
    "    Parameters:\n",
    "    - real_user_id: The real ID of the user for whom we want to generate recommendations.\n",
    "    - clicked_news: DataFrame containing user-item interactions.\n",
    "    - item_profiles: Sparse matrix containing item profiles (TF-IDF values).\n",
    "    - user_to_id: Dictionary mapping real user IDs to internal user IDs.\n",
    "    - id_to_news: Dictionary mapping internal item IDs to real item IDs.\n",
    "    - user_user: Trained UserUser collaborative filtering model.\n",
    "    - n: Number of recommendations to generate.\n",
    "    - w_content: Weight for the content-based model in the hybrid recommendation.\n",
    "    - w_user_user: Weight for the user-user collaborative filtering model in the hybrid recommendation.\n",
    "    \n",
    "    Returns:\n",
    "    A list of n recommended items for the user.\n",
    "    \"\"\"\n",
    "    # Convert real user ID to internal user ID\n",
    "    user_id = user_to_id[real_user_id]\n",
    "    \n",
    "    # Get clicked news by the user\n",
    "    clicked_items = get_clicked_news(real_user_id)\n",
    "    \n",
    "    # Extract item profiles for clicked items\n",
    "    clicked_item_profiles = item_profiles[clicked_items, :]\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = compute_partial_similarity(clicked_item_profiles, item_profiles)\n",
    "    \n",
    "    # Get top-N similar items for each clicked item from content-based model\n",
    "    recommended_items_content_based = set()\n",
    "    for idx, item_id in enumerate(clicked_items):\n",
    "        similar_items = get_top_n_similar(idx, similarity_matrix, n)\n",
    "        recommended_items_content_based.update(similar_items)\n",
    "    \n",
    "    # Remove items that the user has already clicked on\n",
    "    recommended_items_content_based = recommended_items_content_based - set(clicked_items)\n",
    "    \n",
    "    print(f\"content-based recommendations \\n{recommended_items_content_based}\\n\\n\")\n",
    "    \n",
    "    # Get top-N similar items from user-user collaborative filtering model\n",
    "    recommended_items_user_user = recommend_user_user(user_id, n = n*2)  # requesting more to ensure we have enough after merging\n",
    "    \n",
    "    print(f\"useruser recommendations\\n{recommended_items_user_user}\")\n",
    "    \n",
    "    # Create a DataFrame to store the scores from both models\n",
    "    all_recommended_items = pd.DataFrame(list(recommended_items_content_based) + recommended_items_user_user['item'].tolist(), columns=['item'])\n",
    "    all_recommended_items = all_recommended_items.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Add scores from content-based model\n",
    "    all_recommended_items['score_content'] = 0  # initialize with 0\n",
    "    for item_id in recommended_items_content_based:\n",
    "        all_recommended_items.loc[all_recommended_items['item'] == item_id, 'score_content'] = 1  # or another score\n",
    "    \n",
    "    # Add scores from user-user model\n",
    "    all_recommended_items = all_recommended_items.merge(recommended_items_user_user, on='item', how='left').fillna(0)\n",
    "    \n",
    "    # Compute the hybrid score as a weighted sum of the scores from both models\n",
    "    all_recommended_items['hybrid_score'] = w_content * all_recommended_items['score_content'] + w_user_user * all_recommended_items['score']\n",
    "    \n",
    "    # Select the top-N items based on the hybrid score\n",
    "    recommended_items = all_recommended_items.nlargest(n, 'hybrid_score')['item'].tolist()\n",
    "    \n",
    "    # Convert internal item IDs to real item IDs\n",
    "    recommended_real_items = [id_to_news[item_id] for item_id in recommended_items]\n",
    "    \n",
    "    return recommended_real_items[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd3a6ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content-based recommendations \n",
      "{1, 20487, 38928, 45072, 38929, 20502, 28697, 28700, 12316, 4124, 47134, 14381, 24624, 8242, 51, 64, 18497, 22599, 4175, 2127, 10323, 83, 26711, 18522, 18523, 18527, 2144, 41056, 43108, 28774, 8296, 6262, 22649, 39038, 22655, 4225, 12417, 32898, 22663, 6280, 28811, 16529, 8345, 22685, 24735, 32928, 28832, 163, 37030, 8358, 18600, 12456, 28842, 26791, 28843, 16559, 26802, 2232, 189, 35006, 24768, 30924, 2252, 37069, 6349, 32977, 18650, 224, 20706, 8419, 24814, 18671, 20720, 41208, 10488, 16637, 259, 24838, 30985, 14602, 16649, 8461, 28943, 14609, 14615, 41239, 37161, 20778, 45356, 18733, 12590, 10545, 8500, 22837, 8509, 41284, 33092, 35157, 12630, 2389, 6498, 6501, 20840, 45417, 45423, 6511, 39282, 41333, 41342, 14718, 47487, 47490, 2438, 12680, 10636, 22924, 2453, 8597, 4501, 47509, 14745, 16806, 18858, 27052, 39341, 47534, 29103, 6577, 18865, 35254, 6584, 20921, 2491, 4546, 451, 41412, 8647, 43466, 20940, 35277, 43469, 25046, 8664, 23003, 477, 43485, 18911, 6623, 33251, 14821, 37349, 33262, 41454, 31214, 10738, 45556, 6646, 8696, 25080, 45560, 23036, 47614, 4607, 37379, 47622, 6664, 4617, 21001, 35340, 6672, 18961, 21009, 2584, 10779, 12828, 2590, 2595, 41517, 41522, 12851, 16950, 47674, 47675, 33347, 27204, 31303, 27210, 4686, 25168, 8786, 6741, 16981, 602, 31325, 14944, 8801, 4706, 29291, 25198, 17009, 37490, 12921, 35450, 45694, 33408, 23173, 21134, 655, 6801, 10900, 4760, 4767, 675, 15019, 19121, 47794, 4789, 8886, 41659, 19131, 6850, 4803, 15050, 35530, 43725, 35533, 13008, 25298, 25307, 4828, 2780, 43745, 738, 21221, 15083, 2799, 33520, 15097, 763, 17152, 21249, 8961, 2822, 4873, 11021, 15126, 4887, 6938, 39708, 4893, 29479, 35626, 11052, 45868, 21294, 19248, 13109, 9018, 6973, 37694, 45887, 4928, 831, 17217, 29512, 35658, 31564, 17228, 37723, 43873, 11108, 21352, 2930, 33654, 9081, 29562, 2939, 899, 21379, 25477, 33682, 27539, 21403, 5024, 35745, 928, 13228, 19377, 11189, 7099, 35772, 29644, 31696, 11216, 23506, 48085, 33751, 35805, 35811, 23523, 33767, 31728, 3066, 33796, 23558, 31755, 3083, 5132, 37902, 35864, 19484, 39968, 21538, 29731, 21542, 9254, 13353, 1066, 17451, 48172, 25646, 31791, 9266, 29750, 37943, 29754, 39994, 35901, 9285, 5197, 48207, 11343, 1105, 40018, 29783, 5213, 48225, 21604, 40037, 27752, 23663, 1136, 5232, 3188, 5240, 31866, 17533, 27776, 9350, 15499, 29836, 23695, 1167, 9370, 9374, 1185, 15523, 23723, 42157, 5295, 25776, 11444, 38071, 21692, 15549, 38078, 3264, 1217, 15554, 17603, 23749, 9415, 29909, 29911, 40155, 11487, 44268, 40957, 3314, 13558, 34041, 5371, 42238, 5375, 48391, 27913, 34061, 19732, 11546, 25894, 48429, 7484, 32069, 34119, 9548, 17744, 27986, 3411, 34131, 23893, 27991, 7514, 28002, 13672, 48497, 40315, 21884, 1407, 7551, 1408, 42370, 46468, 1414, 30087, 9614, 21910, 44443, 48542, 1453, 44464, 40369, 44465, 13748, 30136, 40377, 34235, 26054, 11718, 5575, 13773, 46542, 30158, 3535, 1487, 3534, 46553, 42459, 30171, 32223, 1504, 38372, 46566, 24043, 3563, 3564, 17902, 44523, 30193, 36340, 7669, 7670, 13827, 38407, 26120, 11785, 22028, 30227, 5652, 13846, 34328, 42522, 15900, 9763, 9767, 36393, 24106, 22058, 15919, 11828, 5686, 28218, 26179, 30279, 26184, 22092, 5710, 24144, 30289, 28242, 5719, 11866, 18011, 28252, 46689, 3683, 24167, 20072, 11891, 22132, 7796, 30324, 36480, 24195, 3716, 22151, 46727, 36492, 30352, 3732, 11924, 11936, 34481, 16055, 9913, 26300, 22209, 34498, 7894, 20188, 34530, 12004, 9956, 9971, 12020, 26360, 16123, 46847, 42754, 26372, 14090, 42770, 7960, 44827, 42786, 28461, 1840, 18225, 22323, 28468, 42810, 14138, 10052, 16199, 3911, 16201, 16207, 10064, 16209, 16210, 16208, 18263, 3930, 28513, 36707, 1891, 12131, 40806, 24422, 26472, 5990, 36712, 44915, 10112, 32641, 3968, 6022, 10123, 44943, 10129, 44950, 16282, 26527, 4001, 6049, 12202, 12211, 36789, 44990, 47041, 18373, 30662, 26568, 30664, 10186, 38862, 32719, 22481, 34773, 42972, 16349, 26596, 22503, 2026, 16363, 14333}\n",
      "\n",
      "\n",
      "useruser recommendations\n",
      "      user  item  prediction\n",
      "0    25103  5632         NaN\n",
      "1    25103   849         NaN\n",
      "2    25103  1657         NaN\n",
      "3    25103  4877         NaN\n",
      "4    25103  7104         NaN\n",
      "..     ...   ...         ...\n",
      "995  25103  7305         NaN\n",
      "996  25103  2472         NaN\n",
      "997  25103  6279         NaN\n",
      "998  25103   385         NaN\n",
      "999  25103  1899         NaN\n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'score'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m recommended_items \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_for_user\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mU53220\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended items for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecommended_items\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 60\u001b[0m, in \u001b[0;36mrecommend_for_user\u001b[1;34m(real_user_id, clicked_news, item_profiles, user_to_id, id_to_news, user_user, n, w_content, w_user_user)\u001b[0m\n\u001b[0;32m     57\u001b[0m all_recommended_items \u001b[38;5;241m=\u001b[39m all_recommended_items\u001b[38;5;241m.\u001b[39mmerge(recommended_items_user_user, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Compute the hybrid score as a weighted sum of the scores from both models\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m all_recommended_items[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m w_content \u001b[38;5;241m*\u001b[39m all_recommended_items[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_content\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m w_user_user \u001b[38;5;241m*\u001b[39m \u001b[43mall_recommended_items\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Select the top-N items based on the hybrid score\u001b[39;00m\n\u001b[0;32m     63\u001b[0m recommended_items \u001b[38;5;241m=\u001b[39m all_recommended_items\u001b[38;5;241m.\u001b[39mnlargest(n, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid_score\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "recommended_items = recommend_for_user('U53220', n = 5)\n",
    "\n",
    "print(f\"Recommended items for user {user_id}: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f7dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
