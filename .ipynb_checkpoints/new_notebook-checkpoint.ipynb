{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d800cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lenskit\n",
    "import lenskit.crossfold as xf\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lenskit.batch import predict\n",
    "from lenskit.metrics.predict import rmse\n",
    "from lenskit.algorithms import Recommender, Predictor\n",
    "from lenskit.algorithms.user_knn import UserUser\n",
    "from lenskit import topn\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d712341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of news items: 48616\n",
      "Number of unique clicked news: 7307\n",
      "Number of unique users: 49445\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "behaviors = pd.read_csv('./small_training_data/behaviors.tsv', delimiter='\\t', header=None)\n",
    "news = pd.read_csv('./small_training_data/news.tsv', delimiter='\\t', header=None)\n",
    "\n",
    "# Naming columns\n",
    "behaviors.columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "news.columns = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "\n",
    "# Remove NaN values in the 'abstract' column\n",
    "news = news.dropna(subset=['abstract']) \n",
    "\n",
    "# Extracting clicked news from behaviors, this is a column of lists of the clicked news (tagget with 1) for each impression\n",
    "behaviors['clicked_news'] = behaviors['impressions'].apply(lambda x: [imp.split('-')[0] for imp in x.split() if imp.split('-')[1] == '1'])\n",
    "\n",
    "# Flattening the clicked news and associating with user_id, that means we divide the lists into one row for each clicked news\n",
    "clicked_news = behaviors.explode('clicked_news')[['user_id', 'clicked_news']].dropna()\n",
    "\n",
    "# Remove clicked news that were removed from the news DataFrame\n",
    "valid_news_ids = set(news['news_id'])\n",
    "# Remove clicked news that were removed from the news DataFrame\n",
    "valid_news_ids = set(news['news_id'])\n",
    "clicked_news = clicked_news[clicked_news['clicked_news'].isin(valid_news_ids)].copy()\n",
    "\n",
    "# Encoding user_id and news_id as categorical variables for memory and computation efficiency\n",
    "clicked_news['user_id'] = clicked_news['user_id'].astype(\"category\")\n",
    "clicked_news['clicked_news'] = clicked_news['clicked_news'].astype(\"category\")\n",
    "\n",
    "print(f\"Total number of news items: {news.shape[0]}\") #48616 unique news\n",
    "print(f\"Number of unique clicked news: {clicked_news['clicked_news'].nunique()}\") #7307 unique news have been clicked\n",
    "print(f\"Number of unique users: {clicked_news['user_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2826e",
   "metadata": {},
   "source": [
    "### Content-based filtering\n",
    "Here we try to pick the best hyperparameter for max_features since with shorter texts, having a large number of features might lead to overly sparse representations and might not capture the meaningful information effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a2cecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 120090\n"
     ]
    }
   ],
   "source": [
    "# Checking the total vocabulary size\n",
    "total_vocabulary = set(word for abstract in news['abstract'] for word in abstract.split())\n",
    "print(f\"Total vocabulary size: {len(total_vocabulary)}\")\n",
    "\n",
    "# Adjusting max_features based on the vocabulary size, we might try different values to check the results\n",
    "max_features = min(1000, len(total_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c874f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create item profiles using the abstract of the news articles.\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=max_features)\n",
    "\n",
    "# Fit and transform the abstracts to create item profiles\n",
    "item_profiles = vectorizer.fit_transform(news['abstract'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b51ae",
   "metadata": {},
   "source": [
    "### Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721d0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'user_id' and 'news_id' are strings\n",
    "clicked_news['user_id'] = clicked_news['user_id'].astype(str)\n",
    "clicked_news['clicked_news'] = clicked_news['clicked_news'].astype(str)\n",
    "\n",
    "# Rename 'clicked_news' column to 'news_id'\n",
    "clicked_news = clicked_news.rename(columns={'clicked_news': 'news_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ddcbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'user_id' and 'news_id' are categorical and encode them as integer codes\n",
    "clicked_news['user_id'] = clicked_news['user_id'].astype(\"category\").cat.codes\n",
    "clicked_news['news_id'] = clicked_news['news_id'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb559b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mappings from encoded IDs to original IDs\n",
    "id_to_user = dict(enumerate(clicked_news['user_id'].astype(\"category\").cat.categories))\n",
    "id_to_news = dict(enumerate(clicked_news['news_id'].astype(\"category\").cat.categories))\n",
    "\n",
    "# Creating reverse mappings from original IDs to encoded IDs\n",
    "user_to_id = {v: k for k, v in id_to_user.items()}\n",
    "news_to_id = {v: k for k, v in id_to_news.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f3a2188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: 49445 \n",
      "items: 7307\n"
     ]
    }
   ],
   "source": [
    "# Create a sparse user-item interaction matrix\n",
    "interaction_matrix = coo_matrix((np.ones(clicked_news.shape[0]), \n",
    "                                 (clicked_news['user_id'], clicked_news['news_id'])))\n",
    "\n",
    "print(f\"users: {interaction_matrix.shape[0]} \\nitems: {interaction_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70918b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Collaborative Filtering Model\n",
    "# Initialize user-user collaborative filtering model\n",
    "user_user = UserUser(15, min_nbrs=3)  # 15 neighbors, minimum 3 neighbors for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ddae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Content-Based Model\n",
    "# Compute item-item similarities from item profiles\n",
    "item_similarities = lil_matrix((item_profiles.shape[0], item_profiles.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and store only the top-k most similar items for each item\n",
    "k = 1\n",
    "\n",
    "for i in range(item_profiles.shape[0]):\n",
    "    # Compute similarities between item i and all other items\n",
    "    similarities = cosine_similarity(item_profiles[i], item_profiles).flatten()\n",
    "    \n",
    "    # Get the top-k most similar items\n",
    "    top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
    "    \n",
    "    # Store the top-k similarities in the sparse matrix\n",
    "    item_similarities[i, top_k_indices] = similarities[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accdee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66532b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
